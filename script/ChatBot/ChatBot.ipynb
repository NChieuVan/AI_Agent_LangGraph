{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3301e22",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, Sequence\n",
    "from langchain_core.messages import BaseMessage, SystemMessage, HumanMessage, ToolMessage,AIMessage\n",
    "from operator import add as add_messages\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.tools import tool\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9aab2339",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4e39262",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(model=\"gpt-4\",api_key=os.getenv(\"OPENAI_API_KEY\") ,temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "52626e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\",api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b45e0a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = r\"../../raw/Stock_Market_Performance_2024.pdf\"\n",
    "\n",
    "if not os.path.exists(pdf_path):\n",
    "    raise FileNotFoundError(f\"PDF file not found at path: {pdf_path}\")\n",
    "\n",
    "pdf_loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "#Check if the PDF was loaded correctly\n",
    "try:   \n",
    "    pages = pdf_loader.load()\n",
    "    if not pages:\n",
    "        raise ValueError(\"No pages were loaded from the PDF.\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error loading PDF: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e104296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of text chunks created: 11\n"
     ]
    }
   ],
   "source": [
    "# Chunking Process\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "\n",
    "pages_plits = text_splitter.split_documents(pages)\n",
    "if not pages_plits:\n",
    "    raise ValueError(\"No text chunks were created from the PDF pages.\")\n",
    "print(f\"Number of text chunks created: {len(pages_plits)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "747e14e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chroma vector store created and persisted at ../../raw/chroma_db with collection name 'stock_market'\n"
     ]
    }
   ],
   "source": [
    "persist_directory = '../../raw/chroma_db'\n",
    "collection_name = 'stock_market'\n",
    "# check persist_directory exists\n",
    "if not os.path.exists(persist_directory):\n",
    "    os.makedirs(persist_directory)\n",
    "\n",
    "try:\n",
    "    vectordb = Chroma.from_documents(\n",
    "        documents=pages_plits, # use the chunked documents\n",
    "        embedding=embeddings, # use the OpenAI embeddings\n",
    "        persist_directory=persist_directory,# specify the directory to persist the database\n",
    "        collection_name=collection_name # name of the collection\n",
    "        )\n",
    "    print(f\"Chroma vector store created and persisted at {persist_directory} with collection name '{collection_name}'\")\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Error creating or persisting Chroma vector store: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "779491c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a retriever from the vector store\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "      search_kwargs={\"k\": 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f4963a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def retriever_tool(query: str) -> str:\n",
    "    \"\"\"Use this tool to answer questions about the content of the PDF document.\"\"\"\n",
    "    docs = retriever.invoke(query)\n",
    "    if not docs:\n",
    "        return \"No relevant documents found.\"\n",
    "    results = []\n",
    "    for i,doc in enumerate(docs):\n",
    "        results.append(f\"Document {i+1}:\\n{doc.page_content}\\n\")\n",
    "    return \"\\n\".join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "285bb757",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [retriever_tool]\n",
    "\n",
    "llm = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c09764b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentState(TypedDict):\n",
    "    messages : Annotated[Sequence[BaseMessage], add_messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c05cd263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def should_continue(state: AgentState) -> bool:\n",
    "    \"\"\"Check if last messages contains tool calls\"\"\"\n",
    "    result = state['messages'][-1]\n",
    "    return hasattr(result, \"tool_calls\") and len(result.tool_calls) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "78232564",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"\n",
    "You are an intelligent AI assistant who answers questions about Stock Market Performance in 2024 based on the PDF document loaded into your knowledge base.\n",
    "Use the retriever tool available to answer questions about the stock market performance data. You can make multiple calls if needed.\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "Please always cite the specific parts of the documents you use in your answers.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d838f9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools_dict = {tool.name: tool for tool in tools}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4cacbf80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'retriever_tool': StructuredTool(name='retriever_tool', description='Use this tool to answer questions about the content of the PDF document.', args_schema=<class 'langchain_core.utils.pydantic.retriever_tool'>, func=<function retriever_tool at 0x0000018872930310>)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "59bc461c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm(state: AgentState) -> AgentState:\n",
    "    \"\"\"Call the LLM with the current state messages\"\"\"\n",
    "    messages = list(state['messages'])\n",
    "    messages = [SystemMessage(content=system_prompt)] + messages\n",
    "    response = llm.invoke(messages)\n",
    "    return {'messages': [response]}"
   ]
  },
  {
   "attachments": {
    "image-2.png": {
     "image/png": "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"
    },
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6a94dcdd",
   "metadata": {},
   "source": [
    "## struct response AIMessage\n",
    "![image.png](attachment:image.png)\n",
    "\n",
    "- \n",
    "![image-2.png](attachment:image-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e44e53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def take_action(state: AgentState) -> AgentState:\n",
    "    \"\"\"Execute tool calls from the LLM's response.\"\"\"\n",
    "\n",
    "    tool_calls = state['messages'][-1].tool_calls\n",
    "    results = []\n",
    "    for t in tool_calls:\n",
    "        print(f\"Calling Tool: {t['name']} with query: {t['args'].get('query', 'No query provided')}\")\n",
    "        \n",
    "        if not t['name'] in tools_dict: # Checks if a valid tool is present\n",
    "            # print(f\"\\nTool: {t['name']} does not exist.\")\n",
    "            result = \"Incorrect Tool Name, Please Retry and Select tool from List of Available tools.\"\n",
    "        \n",
    "        else:\n",
    "            result = tools_dict[t['name']].invoke(t['args'].get('query', ''))\n",
    "            print(f\"Result length: {len(str(result))}\")\n",
    "            \n",
    "\n",
    "        # Appends the Tool Message\n",
    "        results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        results.append(ToolMessage(content=str(result)))\n",
    "\n",
    "    print(\"Tools Execution Complete. Back to the model!\")\n",
    "    return {'messages': results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01178080",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = StateGraph(AgentState)\n",
    "\n",
    "graph.add_node('llm',call_llm)\n",
    "graph.add_node(\"retriever_agent\",take_action)\n",
    "\n",
    "graph.add_conditional_edges(\n",
    "    \"llm\",\n",
    "    should_continue,\n",
    "    {True: \"retriever_agent\", False: END}\n",
    "    \n",
    ")\n",
    "graph.add_edge(\"retriever_agent\", \"llm\")\n",
    "graph.set_entry_point(\"llm\")\n",
    "\n",
    "rag_agent = graph.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8c0471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show image graph\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c84f1766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== RAG AGENT===\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'tool_call_id'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 17\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== ANSWER ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n\u001b[1;32m---> 17\u001b[0m \u001b[43mrunning_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[27], line 11\u001b[0m, in \u001b[0;36mrunning_agent\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m      9\u001b[0m messages \u001b[38;5;241m=\u001b[39m [HumanMessage(content\u001b[38;5;241m=\u001b[39muser_input)] \u001b[38;5;66;03m# converts back to a HumanMessage type\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mrag_agent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== ANSWER ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mcontent)\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\main.py:3026\u001b[0m, in \u001b[0;36mPregel.invoke\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, **kwargs)\u001b[0m\n\u001b[0;32m   3023\u001b[0m chunks: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m|\u001b[39m Any] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3024\u001b[0m interrupts: \u001b[38;5;28mlist\u001b[39m[Interrupt] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 3026\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstream(\n\u001b[0;32m   3027\u001b[0m     \u001b[38;5;28minput\u001b[39m,\n\u001b[0;32m   3028\u001b[0m     config,\n\u001b[0;32m   3029\u001b[0m     context\u001b[38;5;241m=\u001b[39mcontext,\n\u001b[0;32m   3030\u001b[0m     stream_mode\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mupdates\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   3031\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   3032\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m stream_mode,\n\u001b[0;32m   3033\u001b[0m     print_mode\u001b[38;5;241m=\u001b[39mprint_mode,\n\u001b[0;32m   3034\u001b[0m     output_keys\u001b[38;5;241m=\u001b[39moutput_keys,\n\u001b[0;32m   3035\u001b[0m     interrupt_before\u001b[38;5;241m=\u001b[39minterrupt_before,\n\u001b[0;32m   3036\u001b[0m     interrupt_after\u001b[38;5;241m=\u001b[39minterrupt_after,\n\u001b[0;32m   3037\u001b[0m     durability\u001b[38;5;241m=\u001b[39mdurability,\n\u001b[0;32m   3038\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   3039\u001b[0m ):\n\u001b[0;32m   3040\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stream_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalues\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m   3041\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(chunk) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\main.py:2647\u001b[0m, in \u001b[0;36mPregel.stream\u001b[1;34m(self, input, config, context, stream_mode, print_mode, output_keys, interrupt_before, interrupt_after, durability, subgraphs, debug, **kwargs)\u001b[0m\n\u001b[0;32m   2645\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m task \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mmatch_cached_writes():\n\u001b[0;32m   2646\u001b[0m     loop\u001b[38;5;241m.\u001b[39moutput_writes(task\u001b[38;5;241m.\u001b[39mid, task\u001b[38;5;241m.\u001b[39mwrites, cached\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m-> 2647\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m runner\u001b[38;5;241m.\u001b[39mtick(\n\u001b[0;32m   2648\u001b[0m     [t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m loop\u001b[38;5;241m.\u001b[39mtasks\u001b[38;5;241m.\u001b[39mvalues() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m t\u001b[38;5;241m.\u001b[39mwrites],\n\u001b[0;32m   2649\u001b[0m     timeout\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_timeout,\n\u001b[0;32m   2650\u001b[0m     get_waiter\u001b[38;5;241m=\u001b[39mget_waiter,\n\u001b[0;32m   2651\u001b[0m     schedule_task\u001b[38;5;241m=\u001b[39mloop\u001b[38;5;241m.\u001b[39maccept_push,\n\u001b[0;32m   2652\u001b[0m ):\n\u001b[0;32m   2653\u001b[0m     \u001b[38;5;66;03m# emit output\u001b[39;00m\n\u001b[0;32m   2654\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _output(\n\u001b[0;32m   2655\u001b[0m         stream_mode, print_mode, subgraphs, stream\u001b[38;5;241m.\u001b[39mget, queue\u001b[38;5;241m.\u001b[39mEmpty\n\u001b[0;32m   2656\u001b[0m     )\n\u001b[0;32m   2657\u001b[0m loop\u001b[38;5;241m.\u001b[39mafter_tick()\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\_runner.py:162\u001b[0m, in \u001b[0;36mPregelRunner.tick\u001b[1;34m(self, tasks, reraise, timeout, retry_policy, get_waiter, schedule_task)\u001b[0m\n\u001b[0;32m    160\u001b[0m t \u001b[38;5;241m=\u001b[39m tasks[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 162\u001b[0m     \u001b[43mrun_with_retry\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    164\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfigurable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m            \u001b[49m\u001b[43mCONFIG_KEY_CALL\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpartial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                \u001b[49m\u001b[43m_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                \u001b[49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    169\u001b[0m \u001b[43m                \u001b[49m\u001b[43mretry_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretry_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    170\u001b[0m \u001b[43m                \u001b[49m\u001b[43mfutures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweakref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mref\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    171\u001b[0m \u001b[43m                \u001b[49m\u001b[43mschedule_task\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mschedule_task\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m                \u001b[49m\u001b[43msubmit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubmit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m        \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    176\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommit(t, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    177\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\_retry.py:42\u001b[0m, in \u001b[0;36mrun_with_retry\u001b[1;34m(task, retry_policy, configurable)\u001b[0m\n\u001b[0;32m     40\u001b[0m     task\u001b[38;5;241m.\u001b[39mwrites\u001b[38;5;241m.\u001b[39mclear()\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;66;03m# run the task\u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ParentCommand \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[0;32m     44\u001b[0m     ns: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m config[CONF][CONFIG_KEY_CHECKPOINT_NS]\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py:657\u001b[0m, in \u001b[0;36mRunnableSeq.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    655\u001b[0m     \u001b[38;5;66;03m# run in context\u001b[39;00m\n\u001b[0;32m    656\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m set_config_context(config, run) \u001b[38;5;28;01mas\u001b[39;00m context:\n\u001b[1;32m--> 657\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39mrun(step\u001b[38;5;241m.\u001b[39minvoke, \u001b[38;5;28minput\u001b[39m, config, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    659\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m step\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py:401\u001b[0m, in \u001b[0;36mRunnableCallable.invoke\u001b[1;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[0;32m    399\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(ret)\n\u001b[0;32m    400\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunc(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    402\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecurse \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(ret, Runnable):\n\u001b[0;32m    403\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ret\u001b[38;5;241m.\u001b[39minvoke(\u001b[38;5;28minput\u001b[39m, config)\n",
      "Cell \u001b[1;32mIn[23], line 20\u001b[0m, in \u001b[0;36mtake_action\u001b[1;34m(state)\u001b[0m\n\u001b[0;32m     14\u001b[0m         result \u001b[38;5;241m=\u001b[39m tools_dict[t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]]\u001b[38;5;241m.\u001b[39minvoke(t[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124margs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     15\u001b[0m         \u001b[38;5;66;03m# print(f\"Result length: {len(str(result))}\")\u001b[39;00m\n\u001b[0;32m     16\u001b[0m         \n\u001b[0;32m     17\u001b[0m \n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# Appends the Tool Message\u001b[39;00m\n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     results\u001b[38;5;241m.\u001b[39mappend(\u001b[43mToolMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# print(\"Tools Execution Complete. Back to the model!\")\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m'\u001b[39m: results}\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\tool.py:145\u001b[0m, in \u001b[0;36mToolMessage.__init__\u001b[1;34m(self, content, **kwargs)\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    137\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m    138\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Create a ToolMessage.\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \n\u001b[0;32m    141\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;124;03m        content: The string contents of the message.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;124;03m        **kwargs: Additional fields.\u001b[39;00m\n\u001b[0;32m    144\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 145\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(content\u001b[38;5;241m=\u001b[39mcontent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\base.py:72\u001b[0m, in \u001b[0;36mBaseMessage.__init__\u001b[1;34m(self, content, **kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;28mself\u001b[39m, content: Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mlist\u001b[39m[Union[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]]], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any\n\u001b[0;32m     66\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     67\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Pass in content as positional arg.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m \n\u001b[0;32m     69\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;124;03m        content: The string contents of the message.\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(content\u001b[38;5;241m=\u001b[39mcontent, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\load\\serializable.py:130\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32md:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\tool.py:131\u001b[0m, in \u001b[0;36mToolMessage.coerce_args\u001b[1;34m(cls, values)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    129\u001b[0m             values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(x)\n\u001b[1;32m--> 131\u001b[0m tool_call_id \u001b[38;5;241m=\u001b[39m \u001b[43mvalues\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_call_id\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tool_call_id, (UUID, \u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m)):\n\u001b[0;32m    133\u001b[0m     values[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtool_call_id\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(tool_call_id)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'tool_call_id'"
     ]
    }
   ],
   "source": [
    "def running_agent():\n",
    "    print(\"\\n=== RAG AGENT===\")\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nWhat is your question: \")\n",
    "        if user_input.lower() in ['exit', 'quit']:\n",
    "            break\n",
    "            \n",
    "        messages = [HumanMessage(content=user_input)] # converts back to a HumanMessage type\n",
    "\n",
    "        result = rag_agent.invoke({\"messages\": messages})\n",
    "        \n",
    "        print(\"\\n=== ANSWER ===\")\n",
    "        print(result['messages'][-1].content)\n",
    "\n",
    "\n",
    "running_agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2fa9e051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_15764\\3432685414.py:40: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
      "  chatbot=gr.Chatbot(height=500),\n",
      "d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\chat_interface.py:322: UserWarning: The gr.ChatInterface was not provided with a type, so the type of the gr.Chatbot, 'tuples', will be used.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "Could not create share link. Please check your internet connection or our status page: https://status.gradio.app.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\blocks.py\", line 1743, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 749, in async_iteration\n",
      "    return await anext(iterator)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 854, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\chat_interface.py\", line 537, in _wrapper\n",
      "    async for chunk in submit_fn(*args, **kwargs):\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\chat_interface.py\", line 963, in _stream_fn\n",
      "    async for response in generator:\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 743, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 726, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_15764\\3432685414.py\", line 13, in stream_response\n",
      "    for state in rag_agent.stream({\"messages\": history_langchain_format}, stream_mode=\"values\"):\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_15764\\1902621402.py\", line 20, in take_action\n",
      "    results.append(ToolMessage(content=str(result)))\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\tool.py\", line 145, in __init__\n",
      "    super().__init__(content=content, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\base.py\", line 72, in __init__\n",
      "    super().__init__(content=content, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\load\\serializable.py\", line 130, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\tool.py\", line 131, in coerce_args\n",
      "    tool_call_id = values[\"tool_call_id\"]\n",
      "KeyError: 'tool_call_id'\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\queueing.py\", line 715, in process_events\n",
      "    response = await route_utils.call_process_api(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\route_utils.py\", line 322, in call_process_api\n",
      "    output = await app.get_blocks().process_api(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\blocks.py\", line 2220, in process_api\n",
      "    result = await self.call_function(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\blocks.py\", line 1743, in call_function\n",
      "    prediction = await utils.async_iteration(iterator)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 749, in async_iteration\n",
      "    return await anext(iterator)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 854, in asyncgen_wrapper\n",
      "    response = await iterator.__anext__()\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\chat_interface.py\", line 537, in _wrapper\n",
      "    async for chunk in submit_fn(*args, **kwargs):\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\chat_interface.py\", line 963, in _stream_fn\n",
      "    async for response in generator:\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 743, in __anext__\n",
      "    return await anyio.to_thread.run_sync(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\anyio\\to_thread.py\", line 56, in run_sync\n",
      "    return await get_async_backend().run_sync_in_worker_thread(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 2470, in run_sync_in_worker_thread\n",
      "    return await future\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\anyio\\_backends\\_asyncio.py\", line 967, in run\n",
      "    result = context.run(func, *args)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\gradio\\utils.py\", line 726, in run_sync_iterator_async\n",
      "    return next(iterator)\n",
      "  File \"C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_15764\\3432685414.py\", line 13, in stream_response\n",
      "    for state in rag_agent.stream({\"messages\": history_langchain_format}, stream_mode=\"values\"):\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\main.py\", line 2647, in stream\n",
      "    for _ in runner.tick(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\_runner.py\", line 162, in tick\n",
      "    run_with_retry(\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\pregel\\_retry.py\", line 42, in run_with_retry\n",
      "    return task.proc.invoke(task.input, config)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 657, in invoke\n",
      "    input = context.run(step.invoke, input, config, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langgraph\\_internal\\_runnable.py\", line 401, in invoke\n",
      "    ret = self.func(*args, **kwargs)\n",
      "  File \"C:\\Users\\nguye\\AppData\\Local\\Temp\\ipykernel_15764\\1902621402.py\", line 20, in take_action\n",
      "    results.append(ToolMessage(content=str(result)))\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\tool.py\", line 145, in __init__\n",
      "    super().__init__(content=content, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\base.py\", line 72, in __init__\n",
      "    super().__init__(content=content, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\load\\serializable.py\", line 130, in __init__\n",
      "    super().__init__(*args, **kwargs)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\pydantic\\main.py\", line 253, in __init__\n",
      "    validated_self = self.__pydantic_validator__.validate_python(data, self_instance=self)\n",
      "  File \"d:\\App\\Anaconda\\envs\\rag_env\\lib\\site-packages\\langchain_core\\messages\\tool.py\", line 131, in coerce_args\n",
      "    tool_call_id = values[\"tool_call_id\"]\n",
      "KeyError: 'tool_call_id'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    }
   ],
   "source": [
    "def stream_response(message, history):\n",
    "    # Build conversation into LangChain format\n",
    "    history_langchain_format = [system_prompt]\n",
    "    for human, ai in history:\n",
    "        history_langchain_format.append(HumanMessage(content=human))\n",
    "        history_langchain_format.append(AIMessage(content=ai))\n",
    "\n",
    "    if message:\n",
    "        history_langchain_format.append(HumanMessage(content=message))\n",
    "\n",
    "        partial_response = \"\"\n",
    "        # stream qua app (state graph)\n",
    "        for state in rag_agent.stream({\"messages\": history_langchain_format}, stream_mode=\"values\"):\n",
    "            new_msg = state[\"messages\"][-1]  # ly message mi nht\n",
    "\n",
    "            if isinstance(new_msg, AIMessage):\n",
    "                if getattr(new_msg, \"tool_calls\", None):\n",
    "                    partial_response += f\"\\n Gi tool: {new_msg.tool_calls}\"\n",
    "                else:\n",
    "                    partial_response += new_msg.content or \"\"\n",
    "                yield partial_response\n",
    "\n",
    "            elif new_msg.__class__.__name__ == \"ToolMessage\":\n",
    "                partial_response += f\"\\n Tool tr v: {new_msg.content}\"\n",
    "                yield partial_response\n",
    "            # ---------------\n",
    "            # if isinstance(new_msg, AIMessage):\n",
    "            #     partial_response += new_msg.content or \"\"\n",
    "            #     yield partial_response\n",
    "\n",
    "# ---- UI ----\n",
    "demo_interface = gr.ChatInterface(\n",
    "    fn=stream_response,\n",
    "    textbox=gr.Textbox(\n",
    "        placeholder=\"Nhp tin nhn...\",\n",
    "        container=False,\n",
    "        autoscroll=True,\n",
    "        scale=7,\n",
    "    ),\n",
    "    chatbot=gr.Chatbot(height=500),\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_interface.launch(debug=True, share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
